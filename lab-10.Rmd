---
title: "Lab 10 - Grading the professor, Pt. 2"
author: "Fiona Wang"        
date: "March 24th, 2025"
output: github_document
---

## Load packages and data

```{r load-packages, message=FALSE}
library(tidyverse) 
library(tidymodels)
library(openintro)
```

## Exercise 1
Load data
```{r loaddata}
data <- evals
```

Simple linear regression m_bty:
```{r}
m_bty <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ bty_avg, data = data)
tidy(m_bty)
glance(m_bty)
```
The linear model is: score = 0.067(bty_avg) + 3.88.      
R^2 is 0.035 whereas the adjusted R^2 is 0.033. 

### Part 2
## Exercise 2
Multiple regression model
```{r beautygender}
m_bty_gen <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ bty_avg * gender, data = data)
tidy(m_bty_gen)
glance(m_bty_gen)
```

This multiple regression: score = 0.0306(bty_avg) - 0.1835(gender) + 0.080(bty_avg)(gender) + 3.95
The R^2 is 0.071 whereas the adjusted R^2 is 0.065. 

## Exercise 3 (fix from here)
Based on the result from the last exercise, there are three slopes.       
The first slope is 0.0306 for average beauty rating. This means that with one unit increase in the average beauty rating, there is a 0.0306 unit increase in the score while all else is held constant.       
The second slope is -0.1835 for gender. This means that with one unit increase in gender, there is a 0.1835 unit decrease in the score while all else is held constant. As we know from the last lab, female is the reference group, whereas male is 1 unit higher than female. So, male will be 0.1835 lower on average in score than females while all else is held constant.    
The slope for the interaction term is 0.080. This means that there is an interaction effect between average beauty rating and gender. With one unit increase in gender, the slope for average beauty rating will increase 0.08. So, for males, the slope for beauty average rating is 0.1106, while for females, the slope is still 0.0306. 
The intercept is 3.95. This means that when the average beauty rating is 0, and that the professor is a female, this professor will have a rating of 3.95.    

## Exercise 4
According to the R^2 values, 7.1% of the variance in score is explained by the model m_bty_gen, but this is a more optimistic interpretation. If we look at the adjusted R^2, this model explains 6.5% of the variance in score. 

## Exercise 5
For male professors, the line of best fit is:      
score = 0.0306(bty_avg) - 0.1835(gender) + 0.080(bty_avg)(gender) + 3.95     
Since males are gender = 1, we can further write the equation as:      
score = 0.1106(bty_avg) + 3.7665

### Exercise 6
Since I added the interaction term, there is no single answer for this question. For two professors who received 5 for their average beauty rating, males tended to have a higher course evaluation score. For two professors who received 1 for their average beauty rating, females tended to have a higher course evaluation score. 

## Exercise 7
According to our equation, we know that the relationship between beauty average rating and evaluation score is a positive relationship, slope = 0.0306, while all else held constant. However, as indicated by the interaction term, the relationship between beauty average rating and evaluation score increases 0.08. For males, this relationship is stronger than for females. 
## Exercise 8
The adjusted R^2 for m_bty is 0.033, and the adjusted R^2 for m_bty_gen is 0.065. There is an increase in R^2, meaning that gender and the interaction between gender and beauty average scoring explain the evaluation score over and above beauty average score. 

## Exercise 9
In the first model, the parameter estimate is 0.067, whereas in the second model, the parameter estimate is 0.0306. Yes, adding gender and the interaction term changed the slope. 

## Exercise 10
```{r rankandbeauty}
m_bty_rank <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ bty_avg + rank, data = data)
tidy(m_bty_rank)
glance(m_bty_rank)
```

The equation for : score = 0.068(beauty average scoring) - 0.161(tenure track) - 0.126(tenured) + 3.982.    
There are three slopes.       
The first one is 0.068, this means that with one unit increase in the beauty average scoring, there is a 0.068 unit increase in the course evals score, while all else held constant.     
The second slope is -0.161, this means that when someone's rank is tenure track, they will have a 0.161 decrease, on average, in their eval score, while all else held constant.     
The third slope is -0.126, this means that when someone's rank is tenured, they will have a 0.126 decrease, on average, in their eval score, while all else held constant.    
The intercept is 3.982. This means that when someone has the rank of teaching, and have a beauty average score of 0, they will have a 3.982 rating for course evals on average.   

### Part 3
## Exercise 11
While looking at the descriptions for all these variables. I think many of them are not directly linked to course evals, but could indirectly influence students' perceptions of the course and professor.    
Among these, I think ethnicity is the worst predictor. I don't think there will be any association between ethnicity and course eval score. 

## Exercise 12
```{r ethnicity}
m_eth <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ ethnicity, data = data)
tidy(m_eth)
glance(m_eth)
```

According to the output, this model is not significant, and that ethnicity is not a significant predictor of score. 

## Exercise 13
I will not include cls_did_eval as an additional predictor. This is because if we have cls_students and cls_perc_eval, we can infer the number of students in class who completed evaluation. Any variance in score that cls_did_eval is able to explain would have already been explained by cls_perc_eval and cls_students. Adding it will not lead to a inrease in R^2, and itself won't arise as a unique predictor.     

## Exercise 14
```{r everything}
m_all <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ rank + gender + language + age + cls_perc_eval + cls_students + 
        cls_level + cls_profs + cls_credits + ethnicity + bty_avg, data = data)
tidy(m_all)
glance(m_all)
```

An interesting observation is that ethnicity becomes a significant predictor here!?!

## Exercise 15 
Our last model m_all seems to be good enough as it has the highest adjusted R^2 value so far: 0.1412. However, there are some predictors that aren't significant, so I wonder what will happen if I exclude those variables. Below is a improved model. 

```{r improve}
m_all2 <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(score ~ gender + age + cls_perc_eval + cls_credits + ethnicity + bty_avg, data = data)
tidy(m_all2)
glance(m_all2)
```

The adjusted R^2 is even higher for this model: 0.1419. Just higher by a little bit, but this m_all2 model is much more parsimonious with fewer predictors. It makes it easier for me to write out the linear model too. 

score = 3.41 + 0.182(gender) -0.005(age) + 0.005(cls_perc_eval) + 0.532(cls_credits) + 0.241(ethnicity) + 0.064(bty_avg)
